---
name: experiment-infrastructure
description: "Design spec for experiment infrastructure. Use when: setting up experiment infrastructure in a new repo, confused about how suite files or experiment_runner.py work, or need to understand the expected directory structure for results."
---

# Experiment Infrastructure Spec

Infrastructure for running experiments across multiple conditions (models, parameters, etc.) in parallel.

## Why This Structure?

Without enforced structure, experiment results end up scattered everywhere:
- `results/exp1/`, `output/experiment_1/`, `data/results_final/`
- No way to know which code produced which results
- No way to group results from the same run
- Impossible to reproduce or compare experiments

This infrastructure enforces consistent structure with git hash tracking for reproducibility.

## Two Modes

**Standard mode**: General-purpose experiments — training runs, data processing, analysis scripts, anything that writes custom output files.

**Inspect mode**: Inference-only experiments using [Inspect AI](https://inspect.ai-safety-institute.org.uk/). Inspect has its own result format (.eval files) and viewer, so we adapt to its conventions.

| Aspect | Standard | Inspect |
|--------|----------|---------|
| Use case | General experiments | Inference with Inspect AI |
| Directory | Nested per-variant | Flat, all .eval files together |
| Config storage | `config.json` saved by runner | Inside .eval file (by Inspect) |
| Resume support | Yes | No |

**Standard mode** — Nested directories:
```
results/{experiment_name}/{timestamp}_{git_hash}/{variant_name}/config.json
```

**Inspect mode** — Flat directory:
```
results/{experiment_name}/{git_hash}_{variant_name}_{id}.eval
```

## Core Principles

1. **Typed configs** — Suite files use pydantic config classes for validation and autocomplete
2. **Subprocess isolation** — Each experiment runs in a fresh subprocess (clean state)
3. **Runner saves configs** — Configs saved to `output_dir/config.json` BEFORE execution (standard mode)
4. **Enforced results structure** — Runner computes `output_dir` from `variant_name` + `experiment_name`
5. **Dry-run support** — `--dry-run` flag shows what would run without running
6. **Git hash tracking** — Every result is tagged with the commit that produced it

## Project Structure

```
myproject/
├── experiment_config.py        # Copy from reference — BaseExperimentConfig
├── experiment_runner.py        # Copy from reference — ExperimentCase, run_experiments_cli
├── config_my_experiment.py     # Your experiment config (inherits from BaseExperimentConfig)
├── run_my_experiment.py        # Entry point
└── ...

experiments/                    # Separate from project code
└── my-experiment/
    └── suite_my_experiment.py

results/                        # Auto-generated by runner
└── ...
```

## Reference Implementation

Copy both files from this skill's directory (same directory as this SKILL.md file):

1. **`experiment_config.py`** — base config class
   - `BaseExperimentConfig` — inherit from this in your configs

2. **`experiment_runner.py`** — runner infrastructure
   - `ExperimentCase` — pairs a variant name with a config
   - `run_experiments_cli()` — entry point for suite files
   - **Adjust the import** at the top to match your package name

## Config Class Spec

**No global variables.** All experiment parameters must go through config objects to ensure reproducibility.

Config classes contain only experiment parameters — infrastructure metadata like `variant_name` lives in `ExperimentCase` (see Suite File Spec).

### Base Config (provided in experiment_config.py)

`BaseExperimentConfig` provides:
- CLI parsing via pydantic-settings
- Immutability (`frozen=True`)
- `copy_with()` for creating variants (use this instead of `model_copy()`)
- None sentinel handling for CLI args
- Default fields: `output_dir: str` (set by runner), `seed: int = 42`

**Why `output_dir` is protected:** The runner computes `output_dir` from `variant_name` + `experiment_name` + timestamp + git hash. Suite files should never set it directly — attempting to do so via `copy_with()` raises an error.

**Why `_convert_none_sentinel` is required:** CLI args are strings, so there's no native way to represent `None`. Without this validator, if you set `field=None` in a suite file, the runner would skip that arg, and pydantic-settings would use the class default instead — a silent bug. The runner passes `None` values as a sentinel string (`"__none__"`), and this validator converts them back to Python `None`.

**Limitation — dict fields with None:** Dict fields (e.g., `metadata: dict[str, str] | None`) cannot have `None` passed through CLI args (pydantic-settings limitation). The runner will raise an error if you try. To use `None` for a dict field, omit it from `copy_with()` and let the class default apply.

### Experiment-Specific Configs

Inherit from `BaseExperimentConfig`:

```python
# config_my_experiment.py
from myproject.experiment_config import BaseExperimentConfig


class MyExperimentConfig(BaseExperimentConfig):
    model: str = "gpt-4"
    num_samples: int = 100
```

## Entry Point Spec

Entry points (`run_*.py`) are standalone scripts that:
1. Parse config from CLI args (via pydantic-settings)
2. Run the experiment
3. Save results to `config.output_dir`

```python
# run_my_experiment.py
from myproject.config_my_experiment import MyExperimentConfig


def main() -> None:
    config = MyExperimentConfig()
    # config.output_dir is filled in by runner via CLI
    # Example: results/my-experiment/20260128_143052_a1b2c3d/variant-a/

    # ... do work, save results to config.output_dir ...


if __name__ == "__main__":
    main()
```

### Resume Support (Standard Mode)

For entry points to support `--resume`, they must:
1. Check if `config.output_dir` already contains results
2. Skip work that's already done
3. The runner handles directory reuse; the entry point handles idempotency

Example pattern:
```python
def main() -> None:
    config = MyExperimentConfig()

    results_file = Path(config.output_dir) / "results.json"
    if results_file.exists():
        print(f"Already complete: {config.output_dir}")
        return

    # ... do work ...

    results_file.write_text(json.dumps(results))
```

## Suite File Spec

Suite files define experiment cases (variant name + config) and invoke the runner.

`ExperimentCase` separates infrastructure metadata (`variant_name`) from experiment parameters (`config`). This keeps configs focused on experiment parameters and avoids type-checking issues with pydantic-settings CLI parsing.

**Variant names must be unique** — the runner raises an error if duplicate `variant_name` values are found.

```python
# experiments/my-experiment/suite_my_experiment.py
from myproject.config_my_experiment import MyExperimentConfig
from myproject.experiment_runner import ExperimentCase, run_experiments_cli

base_config = MyExperimentConfig()

cases = [
    ExperimentCase(
        variant_name=f"{model}_{setting}",
        config=base_config.copy_with(
            model=model,
            num_samples=100 if setting == "full" else 10,
        ),
    )
    for model in ["model-a", "model-b"]
    for setting in ["full", "quick"]
]

if __name__ == "__main__":
    run_experiments_cli(
        cases,
        entry_point="run_my_experiment.py",
        experiment_name="my-experiment",
        # mode="inspect",  # Add this for Inspect AI experiments
    )
```

## Running Experiments

### CLI Flags

| Flag | Description |
|------|-------------|
| `--dry-run` | Show what would run without running |
| `-p N`, `--parallelism N` | Run N configs in parallel (default: all) |
| `--resume RUN_DIR` | Resume from existing run directory (standard mode only) |

### Basic Usage

```bash
# Always dry-run first
python experiments/my-experiment/suite_my_experiment.py --dry-run

# Run (all cases in parallel by default)
python experiments/my-experiment/suite_my_experiment.py

# Limit to 2 parallel workers (e.g., for rate-limited APIs)
python experiments/my-experiment/suite_my_experiment.py -p 2
```

### Resuming Failed Runs (Standard Mode)

```bash
# First run - some experiments fail
python experiments/my-experiment/suite_my_experiment.py
# Output shows: results/my-experiment/20260128_143052_a1b2c3d/

# Fix the issue, then resume
python experiments/my-experiment/suite_my_experiment.py \
    --resume results/my-experiment/20260128_143052_a1b2c3d
```

The runner reuses the directory; entry points must check for existing results and skip (see Entry Point Spec above).

## Key Points

- **Install the project** — Run `pip install -e .` so imports work from anywhere
- **Use `copy_with()`, not `model_copy()`** — Pydantic's `model_copy()` silently ignores invalid field names
- **`--dry-run` first** — Always verify where results will go before running
- **Entry points handle idempotency** — For resume to work, entry points must skip already-completed work
- **Git hash for reproducibility** — In directory name (standard) or filename (inspect)
- **Enum fields** — Serialized using `.value` when passed through CLI args
